---
title: "Week 7: Web Security"
layout: post
draft: true
---


Week 7: Web Security
====================

Welcome to week 7 of this series. Astute readers may notice that the last
post was [week 5][]. What happened to week 6? Well, it was kind of
dull so I decided not to blog about it. You're welcome.

This week we're talking about web security.

[week 5]: https://magical.github.io/cs373/2018/02/13/week-5

---

When we say "web security" we basically mean "browser security".
web security is important because more and more of our daily lives
happen on the web – you probably check your bank account online, for example.

Traditional software is fading away and being replaced by web apps.
Browsers are subsuming more and more of the capabilities of the operating system,
leaving apps with less incentive to go native.

(It's hilarious to watch brower vendors relearn UI lessons that desktop OSs learned decades ago.
Things like modal dialogs and window management.)

This isn't limited to just legitimate programs –
some modern malware doesn't even try to leave the browser either.
It's happy to just live in your browser and sniff all your web traffic
and not even go to the effort of trying to leverage a browser exploit to leap
onto the underlying OS.
This has advantages for avoiding detection – as i believe we touched on earlier,
most antivirus software is based on file scanning.
If a piece of malware exists only on a webpage, there's no file on your computer to scan.



---

When we think of securing the browser, we usually think of defending it *from*
the web.
But we can't overlook the threats from the other side.

- toolbars
- flash, java
- superfish




User security
----

One of the best ways to attack the web is not to attack the browser but
go directly to the user – social engineering.




- robin sage (social media attacks)


Application security
---


###SQL injection

We discussed SQL injection. I don't have anything to say here that hasn't been covered to death already by a million of online tutorials.
No modern web apps should be vulnerable to SQL injection.
Use parameterized queries.


###XSS

We also discussed XSS, briefly. This is the same category of attack as SQL injection:
someone pasted user input into raw HTML without escaping it.

If you want to avoid SQL injection, XSS, and this entire class of vulnerability,
[read this][eevee] and internalize it:

> A common suggestion is "sanitizing" your input. If someone tells you to do this, punch that person in the face [...]. If you sanitize input, you will almost certainly fuck up legitimate data and still miss a few edge cases. Evil data doesn't need to be "de-eviled"; it needs to be passed across interfaces safely.
> What's an interface? Anywhere two different layers stick together. There's an interface between your program code and your SQL. There's an interface between your Web server and your program. There's an interface between your data and the HTML you generate. Anytime two systems interact, data needs to be transformed from one to the other. The most common security problems arise because the programmer figures "well usernames are just letters" and dumps some mystery data directly into a template or query or whatever.


[eevee]: https://eevee.livejournal.com/330586.html



### Web goat

If you want to play around with web exploits yourself, maybe check out
the [web goat][] - an intentionally vulnerable tutorial-type program that walks
you through exploiting a variety of common vulnerabilities, including XSS
and SQL injection.

On the other hand, it's written in Java so maybe pass.

### SQLiX

It turned out that the lecturer this week was one of the authors of [SQLiX],
a perl program for automatically exploiting SQL injection bugs.
It can do crazy things like identify the type and version of the SQL server that
is running (i.e. MySQL, SQL Server, etc)
and perform *blind SQL injection*, which is where you are able to execute arbitrary
SQL code but *can't see the results* –
the trick is apparently to exfiltrate the data one bit at a time based on whether
you trigger an error or not.

[SQLiX]: https://www.owasp.org/index.php/Category:OWASP_SQLiX_Project


URL classification
------------------

We dipped our toes into URL classification.
Specifically, content-agnostic URL classification, where we look only at the URL and not actually at any of the page content.

How odes this work? It turns out that malicious URLs have a lot in common.
For example, most phishing sites try to make their url look similar to the target site in predictable ways

e.g.

    eu.battle.net.blizzardentertainmentfreeofactivitiese.com

is a phishing url for blizzard's battle.net.

When trying to come up with rules, my main goal was to avoid false positives.
False positives are a big no-no in this sort of automated scanning
because of the "boy who cried wolf" effect –
if your security tool constantly flags benign sites as dangerous,
users will learn to ignore it, or maybe even disable it entirely.

* I also added a rule which gave a boost to well-known tlds like .com, .net, .org, and gave a penalty to tlds like .ru (russia) and .vu (vanuatu).
This is dangerous territory because now we're discriminating based purely on the country that a domain was registered in – a very weak indicator.
Yes there are lots of spam domains with a .ru, but there are plenty of legitimate .ru domains as well.

One reason this works is that people browsing in the US will rarely if ever visit a Russian domain, even a legitimate one.
But presumably people living in russia visit Russian domains more frequently; that is, we can expect statistics about TLD frequency to vary by country.

* Alexa rank is an astonishingly good indicator of a domain's maliciousness.
Alexa basically ranks website popularity.
A low rank (<100) means that a lot of people visit it, and this is a good sign that it is not malicious;
whereas a non-existent or very large rank (>20000) means is a good sign that a url is malicious.
It's not a perfect metric of course – historically the rankings have relied heavily
on stats collected from users with the alexa toolbar installed –
but it performs very well.
The non-existent ranking rule flags around 300 malicious websites in the training set,
with very few false positives.

    URL length is an okay indicator. Some types of attacks rely on feeding a
legitimate but insecure site bad query parameters, for example, which inflates
the URL length. Other times phishers will try to obsfuscate the url with a bunch of garbage.

    It also picks up some false positives, unfortunately.
Plenty of legitimate sites have ugly longass urls,
e.g.

        https://d262ilb51hltx0.cloudfront.net/fit/t/1200/960/gradv/29/81/55/1*tbk3Xr3EK1jJv8RaN9RQ-A.jpeg

* I blacklisted certain words what phishers seemed to target a lot. "paypal" and "googledocs" seemed to be common targets,

* Similarly "wp-admin" and "wp-includes" showed up a lot in hacked wordpress installations. Those got blacklisted as well.


Here's my final score on the training data:

![true positives: 666
true negatives: 1065
false negatives: 268
false positives: 7
accuracy: 86.291127%
](score.png)

Accuracy is the percentage of URLs that were correctly identified –
that is, `(true positives + true negatives) / total`.

Given more time, i'm sure i could have caught more malicious URLs, but
I'm pretty happy with where i ended up – particularly the low number of false positivies.
An accuracy of 85% with almost no false positives is nothing to sneeze at.

Here are all seven of them:

    false positive (1): http://mycountdown.org/countdown.php?cp2_Hex=f5f5f5&cp1_Hex=15c277&img=&hbg=1&fwdt=200&lab=1&ocd=My%20Countdown&text1=happiness&text2=Tanner%20arrives%20in%20Florida&group=My%20Countdown&countdown=My%20Countdown&widget_number=3010&event_time=1402079400&timezone=America/New_York
    false positive (1): http://imp.ad-plus.cn/201306/eba44fc619ecd77b081a37b2a9bc9d6f.php?a=99
    false positive (1): http://js.app.wcdn.cn/ops/game/js/signup/upgrade/toUserUpgrade.minlist.js?v=201310161321
    false positive (2): http://s.kaixin001.com.cn/js/_combo/seclogin,apps*common*AQqLogin,apps*common*AOauthLogin-004835e68.js
    false positive (1): http://cms.singtao.ca/publish/counter.php?Id=5005037
    false positive (1): http://nspt.unitag.jp/c906999ddcc53ef2_576.php
    false positive (2): http://img.uu1001.cn/x/2010-07-22/18-06/2010-07-22_aac7845bbc6ce5cf1b945ebac8f29ff4_0.xml

The mostly ran into a combination of a low alexa rank, use of php, and some other indicator.
I don't know about you but i'm happy to accidentally flag `ad-plus.cn` as malicious.

And actually, they are all old domain names (a weak indicator of non-maliciousness).
By tweaking the weight associated with that heuristic, i was able to get the number
of false positives down to just two.


Acknowledgements
----------------

This post is based off of lecures given by
[Cedric Cochin][] of McAfee Labs.

[Cedric Cochin]: https://www.linkedin.com/in/cochin
